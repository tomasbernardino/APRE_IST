{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#EX 1\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import seaborn as sns\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "RUNS = 10\n",
    "MLP_RANDOM_STATE = 0\n",
    "data = pd.read_csv('parkinsons.csv', delimiter=',')\n",
    "data.head()\n",
    "\n",
    "X, y = data.drop('target', axis=1), np.ravel(data['target'])\n",
    "\n",
    "\n",
    "mae_linear = []\n",
    "mae_mlp_no_activation = []\n",
    "mae_mlp_relu = []\n",
    "\n",
    "\n",
    "for i in range(RUNS):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=i+1)\n",
    "    \n",
    "    # Linear Regression model\n",
    "    linear_regression = LinearRegression()\n",
    "    linear_regression.fit(X_train, y_train)\n",
    "    lr_predictions = linear_regression.predict(X_test)\n",
    "    mae_linear.append(mean_absolute_error(y_test, lr_predictions))\n",
    "    \n",
    "    # MLP model without activation function\n",
    "    mlp_no_activation = MLPRegressor(hidden_layer_sizes=(10,10), activation='identity', random_state=MLP_RANDOM_STATE)\n",
    "    mlp_no_activation.fit(X_train, y_train)\n",
    "    mlp_no_activation_predictions = mlp_no_activation.predict(X_test)\n",
    "    mae_mlp_no_activation.append(mean_absolute_error(y_test, mlp_no_activation_predictions))\n",
    "    \n",
    "    # MLP model with ReLU activation function\n",
    "    mlp_relu = MLPRegressor(hidden_layer_sizes=(10,10), activation='relu', random_state=MLP_RANDOM_STATE)\n",
    "    mlp_relu = mlp_relu.fit(X_train, y_train)\n",
    "    mlp_relu_predictions = mlp_relu.predict(X_test)\n",
    "    mae_mlp_relu.append(mean_absolute_error(y_test, mlp_relu_predictions))\n",
    "    \n",
    "mae_data = pd.DataFrame({\n",
    "    'Linear Regression': mae_linear,\n",
    "    'MLP No Activation': mae_mlp_no_activation,\n",
    "    'MLP ReLU Activation': mae_mlp_relu\n",
    "})\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.boxplot(data=mae_data)\n",
    "plt.title('MAE for Linear Regression and MLP models with and without ReLU activation function')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explanation ex2\n",
    "Activation functions in MLP models enable the representation of non-linear functions, as they introduce the ability to identify non-linear patterns in the data. Without them, the model is simply composed of multiple layers of linear functions, meaning it is strictly a linear model, just like a linear regression model. Therefore, they will have similar outputs, as can be seen in the previous exercise by the similarity between their respective boxplots, which reveal similar MAE values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#EX 3\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.metrics import mean_absolute_error, make_scorer\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "from matplotlib.colors import Normalize\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "MLP_RANDOM_STATE = 0\n",
    "EX7_RANDOM_STATE = 0\n",
    "data = pd.read_csv('parkinsons.csv', delimiter=',')\n",
    "data.head()\n",
    "\n",
    "X, y = data.drop('target', axis=1), np.ravel(data['target'])\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=EX7_RANDOM_STATE)\n",
    "mlp = MLPRegressor(hidden_layer_sizes=(10,10), random_state=MLP_RANDOM_STATE)\n",
    "\n",
    "param_grid = {\n",
    "    'alpha': [0.0001, 0.001, 0.01],            # L2 penalty (regularization term) \n",
    "    'learning_rate_init': [0.001, 0.01, 0.1],\n",
    "    'batch_size': [32, 64, 128]\n",
    "}\n",
    "grid_search = GridSearchCV(estimator=mlp, refit=False, param_grid=param_grid,scoring='neg_mean_absolute_error')\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "best_train_params = grid_search.best_params_\n",
    "best_train_score = -grid_search.best_score_ \n",
    "#print(\"Best train parameters found: \", best_train_params)\n",
    "# This only gives the train MAE for the best combination of hyperparameters\n",
    "# We want the test MAE for all combinations of hyperparameters\n",
    "#print(\"Best train MAE: \", best_train_score)\n",
    "\n",
    "test_results = []\n",
    "test_mae_results = []\n",
    "max_test_mae = float('-inf')\n",
    "min_test_mae = float('inf')\n",
    "\n",
    "for param in grid_search.cv_results_['params']:\n",
    "    # To plot the Test MAE for each combination of hyperparameters\n",
    "    # we need to calculate the Test MAE for each combination's model\n",
    "    mlp.set_params(**param)\n",
    "    mlp.fit(X_train, y_train)\n",
    "    y_pred = mlp.predict(X_test)\n",
    "    \n",
    "    test_mae = mean_absolute_error(y_test, y_pred)\n",
    "    test_results.append({'batch_size': param['batch_size'], 'alpha':param['alpha'], 'learning_rate':param['learning_rate_init'], 'test_mae': test_mae})\n",
    "\n",
    "    if test_mae > max_test_mae:\n",
    "        max_test_mae = test_mae\n",
    "    if test_mae < min_test_mae:\n",
    "        min_test_mae = test_mae\n",
    "\n",
    "# Custom Settings for the heatmaps plot\n",
    "# In order to have the same color scheme in all plots\n",
    "# Since the color depends on the min and max values\n",
    "# We define the same min and max values for all plots\n",
    "\n",
    "norm = Normalize(vmin=min_test_mae, vmax=max_test_mae)\n",
    "results_df = pd.DataFrame(test_results)\n",
    "#print(results_df)\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5), sharey=True)\n",
    "batch_sizes = results_df['batch_size'].unique()\n",
    "\n",
    "for i, batch_size in enumerate(batch_sizes):\n",
    "    # Only use the results for the current batch_size\n",
    "    # because we want a heatmap for each batch size\n",
    "    filtered_df = results_df[results_df['batch_size'] == batch_size]\n",
    "    \n",
    "    pivot_table = filtered_df.pivot_table(values='test_mae', \n",
    "                                          index='alpha', \n",
    "                                          columns='learning_rate')\n",
    "\n",
    "    sns.heatmap(pivot_table, annot=True, fmt=\".3f\", cmap=\"BrBG\",norm=norm, ax=axes[i])\n",
    "    axes[i].set_title(f'Batch Size: {batch_size}')\n",
    "    axes[i].set_xlabel('Learning Rate')\n",
    "    axes[i].set_ylabel('L2 Penalty (alpha)')\n",
    "\n",
    "\n",
    "plt.suptitle('Test MAE for Each Combination of Hyperparameters', fontsize=16)\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
    "\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best combination of hyperparameters found was a learning rate of 0.01, an L2 penalty of 0.001, and a batch size of 32, which resulted in an MAE of 3.651.\n",
    "The learning rate corresponds to how quickly the model updates the weights after each iteration, and it is the factor that most influences the MAE. A low learning rate allows the model to make smaller and more gradual adjustments, reducing the risk of skipping over global minima and, consequently, the magnitude of the MAE, although it increases training time. On the other hand, a high learning rate allows the model to make larger adjustments, but runs the risk of overshooting the global minimum.\n",
    "The L2 penalty is a regularization term that penalizes larger weights, preventing overfitting and stopping the model from propagating noise from the training data to the test data. However, if it is too high, the model may no longer be able to capture complex patterns, causing underfitting.\n",
    "Batch size is the number of samples the model uses to calculate the gradient and update the weights in the direction of the gradient to minimize the error. A small batch size allows the model to make more frequent updates, helping it escape local minima, but with more noise in the training process. A large batch size allows the model to make less frequent updates, but with less noise, which can cause it to get stuck in local minima, leading to slower convergence.\n",
    "The best combination obtained, therefore, has an intermediate learning rate of 0.01. This choice allows the model to make relatively small and gradual adjustments, but without excessively long training times. This value balances convergence time and model accuracy, allowing good error minimization without losing training efficiency. The value found for the L2 penalty, 0.001, is also intermediate, which prevents overfitting while still allowing the model to capture important patterns in the data, so it does not cause underfitting. Additionally, a small batch size of 32 means the model updates more frequently with greater variation in gradients, which helps the model escape local minima, but at the cost of more noise in the training process."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
