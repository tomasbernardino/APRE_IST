{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rpGN8Y3FJ-I9",
        "outputId": "e692e5a6-28b1-456f-865a-e9bf383047d7"
      },
      "outputs": [],
      "source": [
        "#EX 1\n",
        "#a)\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.cluster import KMeans\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "data = pd.read_csv('accounts.csv', delimiter=',')\n",
        "data.head()\n",
        "X_raw = data.iloc[:, :8]\n",
        "#Removing duplicates and null values\n",
        "X_raw = X_raw.drop_duplicates()\n",
        "X_raw = X_raw.dropna()\n",
        "X = pd.get_dummies(X_raw, drop_first=True) # Convert categorical variables to dummy/indicator variables\n",
        "k_values= [2,3,4,5,6,7,8]\n",
        "MAX_ITER = 500\n",
        "RANDOM_STATE = 42\n",
        "\n",
        "scaler = MinMaxScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "sse = []\n",
        "\n",
        "for k in k_values:\n",
        "    kmeans = KMeans(n_clusters=k, max_iter=MAX_ITER, random_state=RANDOM_STATE)\n",
        "    kmeans.fit(X_scaled)\n",
        "    sse.append(kmeans.inertia_)\n",
        "    print(\"inertia: \", kmeans.inertia_)\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(k_values, sse, marker='o')\n",
        "plt.xlabel('Number of clusters (k)')\n",
        "plt.ylabel('Sum of Squared Errors (SSE)')\n",
        "plt.title('SSE vs Number of Clusters')\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OFBGQKWnJ-I_"
      },
      "source": [
        "### Explanation ex1 b)\n",
        "Based on the SSE (Sum of Squared Errors) vs. Number of Clusters (k) plot, we should look for the point where the rate of SSE reduction starts to slow down. This point suggests the optimal number of clusters, as adding more clusters beyond this point does not significantly improve model performance but increases its complexity.\n",
        "\n",
        "As we can see in the plot, there is a sharp drop in SSE when moving from k = 2 to k = 3 (-1572 SSE), indicating that the transition from 2 to 3 clusters greatly improves clustering quality. Similarly, the transition from k = 3 to k = 4, with a decrease of 2197, shows a significant drop in SSE. From k = 4 onwards, the rate of SSE decrease slows down, not exceeding 865 (from k = 4 to k = 5), and continues to decrease more steadily for higher k values, without abrupt changes.\n",
        "\n",
        "Based on the \"elbow finding\" method, we conclude that the inflection point seems to be at k = 4 due to the abrupt change in the rate of SSE decrease. Thus, from this value onward, adding more clusters results in little gain relative to the increased complexity. This represents a good balance between reducing inertia and avoiding unnecessary complexity, which could even lead to overfitting."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8CDL2ghhJ-JA"
      },
      "source": [
        "### Explanation ex1 c)\n",
        "\n",
        "When using k-means, the algorithm is based on calculating distances between points in multidimensional space, usually using Euclidean distance. This works well for numerical variables, where differences between values have real meaning (e.g., the difference between 30 and 50 years). However, for categorical variables (such as \"job type\" or \"education level\"), using k-means can be problematic because it requires transforming these categories into numerical values, for example, through get_dummies(), which increases dimensionality.\n",
        "\n",
        "Moreover, calculating means (used by k-means to update centroids) does not make sense for categorical variables, because it assumes continuity in the data that does not exist in this case.\n",
        "\n",
        "On the other hand, k-modes works with the mode of the categories, i.e., it selects the most frequent category in each cluster. This makes much more sense when dealing with categorical variables, as it does not require calculating means but rather directly compares categories within each feature. However, its calculation is not suitable for numerical variables since we lose the notion of proximity between values in Euclidean space.\n",
        "\n",
        "Therefore, since the subset we are analyzing is mostly composed of categorical features, such as job, marital, and education, among others, and two numerical features (age and balance), we could say that k-modes would be a more suitable clustering method, as it works better with categorical variables, directly comparing categories without distorting distances.\n",
        "However, not knowing the exact importance of each feature, we cannot completely disregard the numerical variables (as would happen with k-modes).\n",
        "\n",
        "Thus, it is probably better to keep k-means with the use of get_dummies() and also apply normalization, to make the behavior of categorical variables closer to continuous variables for calculations.\n",
        "\n",
        "In conclusion, despite the greater computational complexity and the issues mentioned above regarding k-means, the problems of using k-modes may prove more detrimental, so we cannot say that it would be better to use k-modes over k-means."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ldlUnTJOJ-JA",
        "outputId": "b181237f-00f8-4d15-9772-d7e31d8c724e"
      },
      "outputs": [],
      "source": [
        "#EX 2 a)\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "scaler_std = StandardScaler()\n",
        "X_scaled_std = scaler_std.fit_transform(X)\n",
        "\n",
        "pca = PCA(n_components=2)\n",
        "pca.fit(X_scaled_std)\n",
        "X_pca = pca.transform(X_scaled_std)\n",
        "\n",
        "total_variance_explained = pca.explained_variance_ratio_[0] + pca.explained_variance_ratio_[1]\n",
        "percentage_variance_explained = total_variance_explained * 100\n",
        "print(f\"Total variance explained by top 2 components: {percentage_variance_explained:.2f}%\")\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L8DC2ar5J-JA",
        "outputId": "4219f6e7-1a1e-483b-a209-e0710351c4cd"
      },
      "outputs": [],
      "source": [
        "# b)\n",
        "from sklearn.cluster import KMeans\n",
        "import seaborn as sns\n",
        "\n",
        "k_means = KMeans(n_clusters=3, random_state=42)\n",
        "assigned_clusters = k_means.fit_predict(X_scaled_std)\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.scatterplot(x=X_pca[:, 0], y=X_pca[:, 1], hue=assigned_clusters, palette='bright')\n",
        "plt.title(\"K-means Clustering (k=3) on First 2 Principal Components\")\n",
        "plt.xlabel(\"Principal Component 1\")\n",
        "plt.ylabel(\"Principal Component 2\")\n",
        "plt.legend(title=\"Cluster\")\n",
        "plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wQG2Q6DgJ-JA"
      },
      "source": [
        "### Explanation ex2 b)\n",
        "Analyzing the plot, we can notice a relative separation between the 3 clusters and a tendency for the points to move to the right along the cluster. However, there are several regions of the plot where points from different clusters are almost overlapping; that is, if we drew an ellipse around the points of each cluster, these ellipses would overlap. So, although some separation is visible, we cannot say that the clusters are clearly separated. This happens because the two principal components explain only 22.76% of the variance, which is much lower than the recommended 85% to ensure we do not lose part of the data's complexity when reducing dimensionality. Thus, we conclude that the two principal components used are not sufficient to maintain the data's complexity throughout the transformation, which leads to not being able to clearly separate the clusters in the plot.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ct3GKXzcJ-JA",
        "outputId": "a53d21be-3615-46c7-9ef5-3bad6e7fd77e"
      },
      "outputs": [],
      "source": [
        "#2c)\n",
        "import seaborn as sns\n",
        "X = X_raw.copy()\n",
        "df = pd.DataFrame({\n",
        "     'job': X['job'],\n",
        "     'education': X['education'],\n",
        "     'cluster': assigned_clusters  # Cluster assignment from previous question\n",
        "})\n",
        "\n",
        "# Plot for the \"job\" feature\n",
        "\n",
        "sns.displot(data=df, x=\"job\", hue=\"cluster\",\n",
        "            multiple=\"dodge\", stat=\"density\",\n",
        "            shrink=0.8, common_norm=False, aspect=2)\n",
        "plt.title('Density Plot of Job by Cluster', fontsize=14)\n",
        "plt.xlabel(\"Job\", fontsize=13)\n",
        "plt.ylabel(\"Density\", fontsize=13)\n",
        "plt.xticks(rotation=45) \n",
        "plt.show()\n",
        "\n",
        "# Plot for the \"education\" feature\n",
        "sns.displot(data=df, x=\"education\", hue=\"cluster\",\n",
        "            multiple=\"dodge\", stat=\"density\",\n",
        "            shrink=0.8, common_norm=False, aspect=2)\n",
        "plt.title('Density Plot of Education by Cluster')\n",
        "plt.xlabel(\"Education\", fontsize=13)\n",
        "plt.ylabel(\"Density\", fontsize=13)\n",
        "plt.xticks(rotation=45) \n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nuZmT-YGQQps"
      },
      "source": [
        "### Explanation ex2 c)\n",
        "By analyzing the job density plot by cluster, we notice that the \"retired\" category belongs exclusively to cluster 2, meaning all observations in cluster 2 have the job category 'retired'. Looking at the education density plot by cluster, we infer that observations with 'retired' as job predominantly have lower or intermediate education levels, i.e., primary or secondary education.\n",
        "\n",
        "Regarding the other clusters, they show a similar distribution in the frequencies of different jobs, with the main differences being that cluster 0 has a significantly higher density of 'blue-collar' workers compared to cluster 1, while cluster 1 has a significantly higher density of 'student' and 'technician' than cluster 0.\n",
        "For the education plot, in clusters 0 and 1, there is a lower frequency of 'primary' education and a higher frequency of 'secondary' and 'tertiary' compared to cluster 2. However, cluster 0 has a higher density in 'secondary' compared to cluster 1, while cluster 1 has a higher density in 'tertiary'.\n",
        "On the other hand, cluster 2 shows the highest density for 'primary' education.\n",
        "\n",
        "In conclusion, we can observe that observations with the job 'retired' are more likely to belong to cluster 2, meaning they have lower or intermediate education (primary or secondary). The categories 'student' and 'technician' are more likely to belong to cluster 1, meaning they have more advanced education (secondary or tertiary), and finally, the 'blue-collar' category is more likely to belong to cluster 0, which has more intermediate education (mostly secondary, with a smaller difference between tertiary and primary).\n",
        "It is worth noting that, except for cluster 2, the distinctions are not very significant, because, as stated in the previous exercise, the principal components are not able to explain much of the data's complexity."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "teste_apre",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
