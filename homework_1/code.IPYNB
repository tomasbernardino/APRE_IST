{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#EX1\n",
    "\n",
    "from scipy.io.arff import loadarff\n",
    "from sklearn.feature_selection import f_classif\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "diabetes_data = loadarff('diabetes.arff')\n",
    "df = pd.DataFrame(diabetes_data[0])\n",
    "df['Outcome'] = df['Outcome'].str.decode('utf-8')\n",
    "\n",
    "df.head()\n",
    "\n",
    "\n",
    "X = df.drop(columns=['Outcome'],axis=1)\n",
    "y = df['Outcome']\n",
    "\n",
    "f_score, p_values = f_classif(X,y)\n",
    "\n",
    "\n",
    "best_score = max(f_score)\n",
    "worst_score = min(f_score)\n",
    "\n",
    "best_feature = X.columns.values[list(f_score).index(best_score)]\n",
    "worst_feature = X.columns.values[list(f_score).index(worst_score)]\n",
    "print('The best feature is', best_feature)\n",
    "print('The best feature is', worst_feature)\n",
    "\n",
    "\n",
    "plt.figure(figsize=(14, 6))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plot_best = sns.kdeplot(df, x = best_feature ,label='Best_score', hue='Outcome')\n",
    "plt.title(\"Best Discriminative Power Feature Class-Conditional PDF\")\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plot_worst = sns.kdeplot(df, x = worst_feature ,label='Worst_score', hue='Outcome')\n",
    "plt.title(\"Worst Discriminative Power Feature Class-Conditional PDF\")\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#EX2\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import metrics, datasets, tree\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "RANDOM_STATE = 1\n",
    "MINIMUM_SAMPLE_SPLIT = [2, 5, 10, 20, 30, 50, 100]  \n",
    "\n",
    "X = df.drop(columns=['Outcome'],axis=1)\n",
    "y = df['Outcome']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=RANDOM_STATE, stratify=y)\n",
    "\n",
    "\n",
    "test_accuracies = []\n",
    "train_accuracies = []\n",
    "\n",
    "for sample in MINIMUM_SAMPLE_SPLIT:\n",
    "    test_accuracy_avg = 0\n",
    "    train_accuracies_avg = 0\n",
    "\n",
    "    for i in range(10):\n",
    "        predictor = tree.DecisionTreeClassifier(random_state=RANDOM_STATE, min_samples_split=sample)\n",
    "        predictor.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "        y_test_pred = predictor.predict(X_test) #test prediction\n",
    "        \n",
    "        y_train_pred = predictor.predict(X_train)\n",
    "\n",
    "        test_accuracy_avg += metrics.accuracy_score(y_test, y_test_pred)\n",
    "\n",
    "        train_accuracies_avg += metrics.accuracy_score(y_train, y_train_pred)\n",
    "    \n",
    "    test_accuracies.append(test_accuracy_avg/10)\n",
    "    train_accuracies.append(train_accuracies_avg/10)\n",
    "\n",
    "plt.plot(MINIMUM_SAMPLE_SPLIT, test_accuracies, label='Testing Accuracy')\n",
    "plt.plot(MINIMUM_SAMPLE_SPLIT, train_accuracies, label='Training Accuracy')\n",
    "\n",
    "plt.scatter(MINIMUM_SAMPLE_SPLIT, test_accuracies, color='blue', marker='o')\n",
    "plt.scatter(MINIMUM_SAMPLE_SPLIT, train_accuracies, color='orange', marker='o')\n",
    "\n",
    "plt.legend()\n",
    "plt.xlabel('Minimum Sample Split')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Testing and Training Accuracy vs Minimum Sample Split')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.** We can observe that the training data accuracy is higher when the minimum number of samples required to split a node is lower, while the test data accuracy is low. This shows that the model overfits the training data, as it makes decisions based on few samples, making the decision tree very complex and deep, demonstrating poor generalization ability to new data.\n",
    "\n",
    "As the minimum number of samples required to split a node increases, the training data accuracy decreases and the test data accuracy increases, which means the model generalizes better to new data and avoids overfitting. The difference between training and test accuracy decreases, reaching its lowest value when the minimum number of samples is 30.\n",
    "\n",
    "Analyzing the graph, we notice that from 30 samples onwards, the model starts to decrease its accuracy in both training and test data, which shows that the model starts to underfit, meaning the decision tree becomes too simple to correctly predict the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#4.i\n",
    "MAX_DEPTH = 3\n",
    "\n",
    "X = df.drop(columns=['Outcome'],axis=1)\n",
    "y = df['Outcome']\n",
    "\n",
    "predictor = tree.DecisionTreeClassifier(random_state=RANDOM_STATE, max_depth=MAX_DEPTH)\n",
    "predictor.fit(X, y)\n",
    "\n",
    "figure = plt.figure(figsize=(12, 6))\n",
    "tree.plot_tree(predictor, filled=True, feature_names=X.columns, class_names=[\"No Diabetes\", \"Diabetes\"], impurity=False)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4.ii.** The created decision tree has a depth of 3, which means it has 3 levels of decision, making the tree simpler and easier to interpret, avoiding overfitting. It identifies the presence of diabetes based on the provided dataset, associating a set of association rules between the variables to the probability of having diabetes.\n",
    "\n",
    "Thus, we have the following association rules and the posterior probability that symbolizes the probability of having or not having diabetes conditioned on the parameters of the association rule:\n",
    "\n",
    "Rule 1: If the glucose level is less than or equal to 127.5, age less than or equal to 28.5, and BMI is greater than 45.4, the probability of having diabetes is 75%, with 4 training samples. -> Leaf 2\n",
    "\n",
    "Rule 2: If the glucose level is greater than 127.5, BMI is less than or equal to 29.95, and then glucose is greater than 145.5, the probability of having diabetes is 50%, with 35 training samples. -> Leaf 5\n",
    "\n",
    "Rule 3: If the glucose level is greater than 127.5, BMI is greater than 29.95, and glucose is less than or equal to 157.5, the probability of having diabetes is 61%, with 115 training samples. -> Leaf 6\n",
    "\n",
    "Rule 4: If the glucose level is greater than 127.5, BMI is greater than 29.95, and glucose is greater than 157.5, the probability of having diabetes is 87%, with 92 training samples. -> Leaf 7\n",
    "\n",
    "In conclusion, based on our dataset, diabetes is characterized by a high glucose level associated with a high BMI, as we can see from association rules 2, 3, and 4, associated with leaves 5, 6, and 7, respectively. Regarding rule 1, the sample is too small to draw conclusions and may just be noise in the data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
